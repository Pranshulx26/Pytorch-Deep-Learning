{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84ae04be-aa74-42d6-b268-83f33735a00c",
   "metadata": {},
   "source": [
    "# What is `torch.nn` *really*?\n",
    "\n",
    "* Notebook Link: https://docs.pytorch.org/tutorials/beginner/nn_tutorial.html\n",
    "* \n",
    "**Authors:** Jeremy Howard, [fast.ai](https://www.fast.ai)  \n",
    "Thanks to Rachel Thomas and Francisco Ingham.\n",
    "\n",
    "PyTorch provides the elegantly designed modules and classes  \n",
    "- [`torch.nn`](https://pytorch.org/docs/stable/nn.html)  \n",
    "- [`torch.optim`](https://pytorch.org/docs/stable/optim.html)  \n",
    "- [`Dataset`](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset)  \n",
    "- [`DataLoader`](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)  \n",
    "\n",
    "to help you create and train neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "In order to fully utilize their power and customize them for your problem, you need to **really understand exactly what they're doing**.  \n",
    "\n",
    "To develop this understanding, we will:\n",
    "\n",
    "1. First train a basic neural net on the MNIST dataset **without** using any features from these models—only using basic PyTorch tensor functionality.\n",
    "2. Then incrementally add one feature at a time from:\n",
    "   - `torch.nn`\n",
    "   - `torch.optim`\n",
    "   - `Dataset`\n",
    "   - `DataLoader`  \n",
    "   \n",
    "...showing **exactly what each piece does**, and how it makes the code either:\n",
    "- more concise, or\n",
    "- more flexible.\n",
    "\n",
    "---\n",
    "\n",
    "> **This tutorial assumes you already have PyTorch installed and are familiar with the basics of tensor operations.**  \n",
    "> (If you're familiar with NumPy array operations, you'll find the PyTorch tensor operations nearly identical.)\n",
    "\n",
    "---\n",
    "\n",
    "## MNIST data setup\n",
    "\n",
    "We will use the classic [MNIST](https://yann.lecun.com/exdb/mnist/index.html) dataset,  \n",
    "which consists of **black-and-white images of hand-drawn digits (0 to 9).**\n",
    "\n",
    "We will use:\n",
    "- [`pathlib`](https://docs.python.org/3/library/pathlib.html) for handling file paths (Python 3 standard library),\n",
    "- and [`requests`](http://docs.python-requests.org/en/master/) to download the dataset.\n",
    "\n",
    "We will **only import modules when we use them**, so you can see exactly what’s being used at each point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a715a19-501c-447a-a70f-8cc113fcd778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93ee8157-2f4f-414a-a6ce-944b08b71503",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('data')\n",
    "PATH = DATA_PATH / 'mnist'\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"https://github.com/pytorch/tutorials/raw/main/_static/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3702119-47e0-43aa-b1a0-3eb4d6f8ab53",
   "metadata": {},
   "source": [
    "> 📦 **Note:**  \n",
    "> This dataset is in **NumPy array format** and has been stored using **pickle**,  \n",
    "> which is a Python-specific format for **serializing data**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab0a77de-4802-4b86-9fb6-a10749da0520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), 'rb') as f:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38301bf8-6700-43ea-bc06-4f4ecd7ef03c",
   "metadata": {},
   "source": [
    "> Each image is 28 x 28, and is being stored as a flattened row of length \n",
    "> 784 (=28x28). Let's take a look at one; we need to reshape it to 2d\n",
    "> first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58d24e29-4f4f-46f8-85ee-cbec5c0283ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "plt.imshow(x_train[0].reshape((28, 28)), cmap='gray')\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7294a7-9994-485b-8104-53b44e0659a4",
   "metadata": {},
   "source": [
    "PyTorch uses torch.tensor, rather than numpy arrays, so we need to convert our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7114f8ed-6860-4690-bfda-146a4a72af57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n",
      "torch.Size([50000, 784])\n",
      "tensor(0) tensor(9)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x_train, y_train, x_valid, y_valid = map(torch.tensor, (x_train, y_train, x_valid, y_valid))\n",
    "n, c = x_train.shape\n",
    "print(x_train, y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9395eb4b-1bef-4792-96a0-486dfc726770",
   "metadata": {},
   "source": [
    "## Neural Net from Scratch (without `torch.nn`)\n",
    "\n",
    "Let's first create a model using nothing but **PyTorch tensor operations**.  \n",
    "We're assuming you're already familiar with the basics of neural networks.  \n",
    "(If you're not, you can learn them at [course.fast.ai](https://course.fast.ai)).\n",
    "\n",
    "PyTorch provides methods to create **random** or **zero-filled tensors**,  \n",
    "which we will use to create our **weights** and **bias** for a simple linear model.  \n",
    "These are just regular tensors, with one very special addition:  \n",
    "we tell PyTorch that they **require a gradient**.  \n",
    "This causes PyTorch to **record all operations** done on the tensor,  \n",
    "so that it can calculate the **gradient during back-propagation automatically**!\n",
    "\n",
    "> 📌 For the weights, we set `requires_grad` **after** the initialization,  \n",
    "> since we don't want that step included in the gradient.  \n",
    "> (Note: a trailing `_` in PyTorch signifies that the operation is performed **in-place**.)\n",
    "\n",
    "> 📝 **Note:**  \n",
    "> We are initializing the weights here with  \n",
    "> [Xavier Initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)  \n",
    "> (by multiplying with `1/sqrt(n)`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "699199aa-0768-402a-b8b4-ead4739c1c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46b74cc-a97f-44ce-8846-0dd6ca8deb8a",
   "metadata": {},
   "source": [
    "Thanks to **PyTorch’s ability to calculate gradients automatically**,  \n",
    "we can use any standard **Python function (or callable object)** as a model! ✅\n",
    "\n",
    "So let’s just write a **plain matrix multiplication** and **broadcasted addition**  \n",
    "to create a simple **linear model**.\n",
    "\n",
    "We also need an **activation function**,  \n",
    "so we’ll write `log_softmax` and use it.\n",
    "\n",
    "---\n",
    "\n",
    "🧠 **Remember:**  \n",
    "Although PyTorch provides **lots of prewritten loss functions**,  \n",
    "**activation functions**, and so forth,  \n",
    "you can **easily write your own** using **plain Python**.\n",
    "\n",
    "⚡ PyTorch will even create **fast accelerator** (like CUDA)  \n",
    "or **vectorized CPU code** for your function **automatically**!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cc7da51-3779-4d43-a5f9-7d9645d6077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54545e9-8849-4341-b06f-0672bf323b47",
   "metadata": {},
   "source": [
    "In the above, the `@` stands for the **matrix multiplication** operation.\n",
    "\n",
    "We will call our function on **one batch of data**  \n",
    "(in this case, **64 images**).  \n",
    "This is called **one forward pass**. 🔁\n",
    "\n",
    "---\n",
    "\n",
    "💡 **Note:**  \n",
    "Our predictions won’t be any better than **random** at this stage,  \n",
    "since we start with **random weights**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0b5af14-d177-4fbc-9254-9a8c8313bcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.4005, -1.7608, -2.6295, -1.8237, -2.4650, -2.5422, -2.1644, -2.7311,\n",
      "        -2.3976, -2.6695], grad_fn=<SelectBackward0>) torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "bs = 64 # batch size\n",
    "xb = x_train[0:bs] # a mini-batch from x\n",
    "preds = model(xb)\n",
    "preds[0], preds.shape\n",
    "print(preds[0], preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca06c68-8718-49ee-9f3c-58b0cdeebaad",
   "metadata": {},
   "source": [
    "As you see, the preds tensor contains not only the tensor values, but also a gradient function. We’ll use this later to do backprop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c085b510-c29a-45e4-8f8a-447545d10ff5",
   "metadata": {},
   "source": [
    "Let’s implement negative log-likelihood to use as the loss function (again, we can just use standard Python):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd06ae1f-4d9b-4ee6-b8f7-990858824638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_func = nll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aad59f-a702-404b-b05b-008d63f5c9b4",
   "metadata": {},
   "source": [
    "Let’s check our loss with our random model, so we can see if we improve after a backprop pass later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fafdf688-35e0-4ca8-bc8d-be2b96e19bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4419, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "yb = y_train[0:bs]\n",
    "print(loss_func(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dadb6d-4de2-4457-a1f8-e3de665608a9",
   "metadata": {},
   "source": [
    "Let’s also implement a function to calculate the accuracy of our model. For each prediction, if the index with the largest value matches the target value, then the prediction was correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c539ded1-2dfd-4b13-8057-36933572a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20924137-c425-4ee7-bca1-f12f9d498913",
   "metadata": {},
   "source": [
    "Let’s check the accuracy of our random model, so we can see if our accuracy improves as our loss improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcec1dbd-8903-4911-be40-bc53df5daa13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0469)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588480ef-4b11-4882-8dcd-3e20a88b3032",
   "metadata": {},
   "source": [
    "We can now run a **training loop**. For each iteration, we will:\n",
    "\n",
    "- Select a **mini-batch** of data (of size `bs`)\n",
    "- Use the **model** to make predictions\n",
    "- Calculate the **loss**\n",
    "- Call `loss.backward()` to compute gradients for the model's **weights** and **bias**\n",
    "\n",
    "---\n",
    "\n",
    "🔁 **Weight Update Step:**\n",
    "\n",
    "We use these gradients to **update the weights and bias**.  \n",
    "This is done inside a `torch.no_grad()` context manager,  \n",
    "since we don’t want these operations to be tracked by **autograd**.\n",
    "\n",
    "📖 Learn more about how PyTorch’s Autograd works:  \n",
    "[PyTorch Autograd documentation](https://pytorch.org/docs/stable/notes/autograd.html)\n",
    "\n",
    "---\n",
    "\n",
    "🧹 **Resetting Gradients:**\n",
    "\n",
    "We reset gradients to **zero** using `optimizer.zero_grad()` (or `weights.grad.zero_()`),  \n",
    "because `loss.backward()` **adds** to existing gradients instead of replacing them.\n",
    "\n",
    "---\n",
    "\n",
    "💡 **Tip:**  \n",
    "You can use the **standard Python debugger** to step through PyTorch code  \n",
    "and inspect variable values.  \n",
    "To try it out, uncomment and use:  \n",
    "```python\n",
    "import pdb; pdb.set_trace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "853a3e17-1090-4862-91d5-b59ba806b9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c3f354f-1c29-4a89-9878-633208a58fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32mc:\\users\\yashs\\appdata\\local\\temp\\ipykernel_7288\\1115188063.py\u001b[0m(7)\u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NameError: name 'loss' is not defined\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  pred\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NameError: name 'pred' is not defined\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0051,  0.0212,  0.0411,  ..., -0.0410,  0.0648, -0.0013],\n",
      "        [ 0.0514, -0.0090, -0.0101,  ..., -0.0499, -0.0170, -0.0255],\n",
      "        [-0.0189, -0.0232,  0.0575,  ...,  0.0567, -0.0402, -0.0018],\n",
      "        ...,\n",
      "        [ 0.0084,  0.0795, -0.0045,  ...,  0.0064, -0.0056, -0.0600],\n",
      "        [-0.0126,  0.0268,  0.0087,  ...,  0.0572, -0.0128,  0.0173],\n",
      "        [ 0.0386,  0.0109, -0.0695,  ...,  0.0024,  0.0293, -0.0060]],\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    }
   ],
   "source": [
    "lr = 0.5 # learning rate\n",
    "epochs = 2 # how many epochs to train for\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n-1)//bs+1):\n",
    "        set_trace()\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr \n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0ea6c9-f54c-4533-bf5f-f6415faac26c",
   "metadata": {},
   "source": [
    "That’s it: we’ve created and trained a minimal neural network (in this case, a logistic regression, since we have no hidden layers) entirely from scratch!\n",
    "\n",
    "Let’s check the loss and accuracy and compare those to what we got earlier. We expect that the loss will have decreased and accuracy to have increased, and they have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1271619-1d6a-418e-8953-a6dd1f4de980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4419, grad_fn=<NegBackward0>) tensor(0.0469)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938aedd9-968a-4367-b418-fc430e23b2de",
   "metadata": {},
   "source": [
    "## Refactoring with `torch.nn.functional`\n",
    "\n",
    "We will now **refactor** our code so that it does the same thing as before,  \n",
    "but we’ll start leveraging PyTorch’s `nn` modules to make it:\n",
    "\n",
    "- ✅ Shorter  \n",
    "- ✅ More understandable  \n",
    "- ✅ More flexible  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 Step 1: Replace Hand-Written Functions\n",
    "\n",
    "The **first and easiest** step is to replace our **custom activation and loss functions**  \n",
    "with those provided by `torch.nn.functional`, which is commonly imported as:\n",
    "\n",
    "```python\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6800355a-883f-4f80-8891-3efa15b055a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weights + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb6a54f-e67c-49ca-9afa-31d0258c3e30",
   "metadata": {},
   "source": [
    "Note that we no longer call log_softmax in the model function. Let’s confirm that our loss and accuracy are the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c626d8d2-231f-4ab2-85d9-b67e97ae000d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4419, grad_fn=<NllLossBackward0>) tensor(0.0469)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f948dd25-6edc-4890-ac2f-a094292dfe09",
   "metadata": {},
   "source": [
    "## Using `nn.Module` and `nn.Parameter`\n",
    "\n",
    "Next up, we’ll use:\n",
    "\n",
    "- `nn.Module`\n",
    "- `nn.Parameter`\n",
    "\n",
    "to build a **clearer and more concise** training loop.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧱 Subclassing `nn.Module`\n",
    "\n",
    "We will **subclass** `nn.Module` to define our model.  \n",
    "This class:\n",
    "\n",
    "- ✅ Stores our model’s **weights and bias**\n",
    "- ✅ Defines the model’s **forward pass**\n",
    "- ✅ Keeps track of all **trainable parameters**\n",
    "\n",
    "```python\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # define parameters here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define forward pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae85467b-2821-4dd2-87a2-7525f45ae36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10)/ math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f6c8bc-7861-47e6-ae11-991c1bc3cdd0",
   "metadata": {},
   "source": [
    "since we’re now using an object instead of just using a function, we first have to instantiate our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91f89712-02e7-44b3-9bcd-ac93802a3a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mnist_Logistic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc2d6b7-a322-4b9f-aefa-7b090c3bc121",
   "metadata": {},
   "source": [
    "Now we can calculate the loss in the same way as before. Note that nn.Module objects are used as if they are functions (i.e they are callable), but behind the scenes Pytorch will call our forward method automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e252fff1-9af7-4e0f-a0ae-e14f9a9fc86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3782, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c59263-3d1b-4e94-b9cf-69e03214b4d1",
   "metadata": {},
   "source": [
    "#### Making the Training Loop More Concise with `nn.Module`\n",
    "\n",
    "---\n",
    "\n",
    "### 🧾 Previously...\n",
    "\n",
    "When we used **plain tensors** for `weights` and `bias`, we had to update and reset their gradients **manually**:\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    weights -= weights.grad * lr\n",
    "    bias -= bias.grad * lr\n",
    "    weights.grad.zero_()\n",
    "    bias.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cb93e7-60b8-4fec-8a44-092dceaae6a8",
   "metadata": {},
   "source": [
    "Now we can take advantage of `model.parameters()` and `model.zero_grad()` (which are both defined by PyTorch for `nn.Module`) to make those steps more concise and less prone to the error of forgetting some of our parameters, particularly if we had a more complicated model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7fe72a4-48eb-4031-97e8-dd09f60c60ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     for p in model.parameters(): p -= p.grad * lr\n",
    "#     model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8957aa61-f047-4bcb-a283-3c873e940184",
   "metadata": {},
   "source": [
    "We’ll wrap our little training loop in a fit function so we can run it again later.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08c80cc5-837e-4747-bf93-9c72f05d08fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n - 1) // bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "\n",
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc6959c-ed30-4993-b20f-4bd4511d10d4",
   "metadata": {},
   "source": [
    "Let’s double-check that our loss has gone down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cea3984f-ea4a-4c8d-9650-242be51e8e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2266, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abb3c85-5bbd-4e36-815f-66f27440e471",
   "metadata": {},
   "source": [
    "### Refactor using `nn.Linear`\n",
    "\n",
    "We continue to refactor our code. Instead of manually defining and initializing `self.weights` and `self.bias`, and calculating `xb @ self.weights + self.bias`, we will instead use the PyTorch class `nn.Linear` for a linear layer, which does all that for us.\n",
    "\n",
    "PyTorch has many types of predefined layers that can greatly simplify our code, and often make it faster too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f78ee07-a430-44c3-9b18-fd2bec4d9cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa4c4bbe-2e95-456d-88b4-f0f90c79bee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3227, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = Mnist_Logistic()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dc94ed-6af0-4f6c-b5a7-dd5974ed470a",
   "metadata": {},
   "source": [
    "We are still able to use our same fit method as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d41ff96-6889-41df-a02e-25322fb7c967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1931, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "fit()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fe7182-4404-492e-9fb3-3814bf5766ea",
   "metadata": {},
   "source": [
    "### Refactor using `torch.optim`\n",
    "\n",
    "PyTorch also has a package with various optimization algorithms, `torch.optim`. We can use the `step` method from our optimizer to take a forward step, instead of manually updating each parameter.\n",
    "\n",
    "This will let us replace our previous manually coded optimization step:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cff3b2-bdeb-4d96-8551-bb4ceadf4b2f",
   "metadata": {},
   "source": [
    "optim.zero_grad() resets the gradient to 0 and we need to call it before computing the gradient for the next minibatch.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af449d4d-f45b-4b9b-96fa-34f61ab86a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb92e1e6-9908-46c8-a1bf-953941d6c7b0",
   "metadata": {},
   "source": [
    "We’ll define a little function to create our model and optimizer so we can reuse it in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e269d30-5e3a-4255-9a4e-4b8b127141a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3337, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "model, opt = get_model()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3df2334a-7f3b-4a1f-b857-1eb584deb0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0834, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26474f9-fc05-4ef4-a8b3-ca25baa0b023",
   "metadata": {},
   "source": [
    "### Refactor using `Dataset`\n",
    "---\n",
    "\n",
    "PyTorch has an abstract `Dataset` class. A `Dataset` can be anything that has a `__len__` function (called by Python's standard `len` function) and a `__getitem__` function as a way of indexing into it.\n",
    "\n",
    "[This tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) walks through a nice example of creating a custom `FacialLandmarkDataset` class as a subclass of `Dataset`.\n",
    "\n",
    "PyTorch's [`TensorDataset`](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset) is a `Dataset` wrapping tensors. By defining a length and way of indexing, this also gives us a way to iterate, index, and slice along the first dimension of a tensor. This will make it easier to access both the independent and dependent variables in the same line as we train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "920381d3-9ebd-4851-befd-0a6931a491ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1179dc2-c0d8-4ed0-9053-51bac16595f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "6243207c-5a27-4dc6-be47-7ab62aa64ad1",
   "metadata": {},
   "source": [
    "Both x_train and y_train can be combined in a single TensorDataset, which will be easier to iterate over and slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dc77e475-32af-492b-8136-35e0f9d082fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923d7393-cd3d-4f5e-856c-82a6f04e7d28",
   "metadata": {},
   "source": [
    "previously, we had to iterate through minibatches of x and y values separately:\n",
    "```python\n",
    "xb = x_train[start_i:end_i]\n",
    "yb = y_train[start_i:end_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c263f2c3-54c3-42b0-a607-3c8b95413dcc",
   "metadata": {},
   "source": [
    "Now we can do these two steps together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb50831c-542c-4fb8-ad22-18d01a7da21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = train_ds[i*bs : i*bs+bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6dcaa180-0d9d-40eb-b12e-bd17318be263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0807, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        xb, yb = train_ds[i * bs: i * bs + bs]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee787fe0-7b10-47ca-a216-d79a04570712",
   "metadata": {},
   "source": [
    "### Refactor using `DataLoader`\n",
    "\n",
    "PyTorch’s `DataLoader` is responsible for managing batches. You can create a `DataLoader` from any `Dataset`. `DataLoader` makes it easier to iterate over batches. \n",
    "\n",
    "Rather than having to use `train_ds[i*bs : i*bs+bs]`, the `DataLoader` gives us each minibatch automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "593b7427-5e36-4291-933c-8a5b55725db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5efe510-856a-4137-9116-68f41f0ede36",
   "metadata": {},
   "source": [
    "Previously, our loop iterated over batches (xb, yb) like this:\n",
    "\n",
    "```python\n",
    "for i in range((n-1)//bs+1):\n",
    "    xb, yb = train_ds[i*bs : i*bs+bs]\n",
    "    pred = model(xb)\n",
    "```\n",
    "Now, our loop is much cleaner, as (xb, yb) are loaded automatically from the data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c9224ae-7007-4a8c-b460-566abb58ddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for xb, yb in train_dl:\n",
    "    pred = model(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c6f8bb5-2478-42dc-9adb-8b2cfa145473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0821, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78731a4-1181-489a-854f-f546888736b5",
   "metadata": {},
   "source": [
    "### Thanks to PyTorch’s `nn.Module`, `nn.Parameter`, `Dataset`, and `DataLoader`, our training loop is now dramatically smaller and easier to understand.\n",
    "\n",
    "Let’s now try to add the basic features necessary to create effective models in practice.\n",
    "\n",
    "---\n",
    "\n",
    "### Add Validation\n",
    "\n",
    "In section 1, we were just trying to get a reasonable training loop set up for use on our training data. In reality, you should always also have a **validation set**, in order to identify if you are overfitting.\n",
    "\n",
    "- **Shuffling the training data** is important to prevent correlation between batches and overfitting.  \n",
    "- On the other hand, the **validation loss** will be identical whether we shuffle the validation set or not. Since shuffling takes extra time, it makes no sense to shuffle the validation data.\n",
    "\n",
    "We’ll use a **batch size for the validation set** that is **twice as large** as that for the training set. This is because the validation set does not need backpropagation and thus takes **less memory** (it doesn’t need to store the gradients). We take advantage of this to use a larger batch size and compute the loss more quickly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6a0e2271-ec99-441a-b7e8-041c9b4cf0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=2 * bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4c4f2f-bc41-4820-b64f-4c8738abe2f5",
   "metadata": {},
   "source": [
    "We will calculate and print the validation loss at the end of each epoch.\n",
    "\n",
    "(Note that we always call `model.train()` before training, and `model.eval()` before inference, because these are used by layers such as `nn.BatchNorm2d` and `nn.Dropout` to ensure appropriate behavior for these different phases.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "008bc574-fbc2-412b-9287-f64bc4176c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.3038)\n",
      "1 tensor(0.2934)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n",
    "    print(epoch, valid_loss/len(valid_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9a9e53-5d3c-4229-a2fd-9d34e298e8e6",
   "metadata": {},
   "source": [
    "### Create `fit()` and `get_data()`\n",
    "\n",
    "We’ll now do a little **refactoring** of our own. Since we go through a similar process twice — calculating the loss for both the **training set** and the **validation set** — let’s make that into its own function, `loss_batch`, which computes the loss for one batch.\n",
    "\n",
    "We pass an **optimizer** in for the **training set**, and use it to perform **backpropagation**.  \n",
    "For the **validation set**, we **don’t pass an optimizer**, so the method doesn’t perform backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c3ab1e0e-eb0e-49bd-9711-a05a1b984c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e006c-4f47-494d-8c8a-73eba8fbbf92",
   "metadata": {},
   "source": [
    "fit runs the necessary operations to train our model and compute the training and validation losses for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4cb2d798-1810-4465-a1dd-05f8572ebeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(*[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl])\n",
    "\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9b3579-6eec-4fa8-987e-135658f77d01",
   "metadata": {},
   "source": [
    "get_data returns dataloaders for the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ff3717c-e4d9-40f0-b706-7138b9cead80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbaafd8-2eae-4a29-bd31-989c84d6ba3e",
   "metadata": {},
   "source": [
    "Now, our whole process of obtaining the data loaders and fitting the model can be run in 3 lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2772fbce-a5e9-4892-bf2d-c0f87bc62e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3462883511543274\n",
      "1 0.28890423713922503\n"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "model, opt = get_model()\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33791f0-fc82-46b9-a48f-bde381f19989",
   "metadata": {},
   "source": [
    "You can use these basic 3 lines of code to train a wide variety of models. Let’s see if we can use them to train a convolutional neural network (CNN)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98965e64-bbb2-481a-8221-0924bd51df76",
   "metadata": {},
   "source": [
    "### Switch to CNN\n",
    "\n",
    "We are now going to build our neural network with **three convolutional layers**.  \n",
    "Because none of the functions in the previous section assume anything about the model form, we’ll be able to use them to train a **CNN without any modification**.\n",
    "\n",
    "We will use PyTorch’s predefined `Conv2d` class as our **convolutional layer**.  \n",
    "We define a CNN with 3 convolutional layers. Each convolution is followed by a **ReLU**.  \n",
    "At the end, we perform an **average pooling**.  \n",
    "(Note that `view` is PyTorch’s version of Numpy’s `reshape`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "34459b30-6cfb-4d69-9095-e0560e917955",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "        xb = F.relu(self.conv3(xb))\n",
    "        xb = F.avg_pool2d(xb, 4)\n",
    "        return xb.view(-1, xb.size(1))\n",
    "\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9222c4ad-a966-4ff5-858e-7efbae633fa5",
   "metadata": {},
   "source": [
    "Momentum is a variation on stochastic gradient descent that takes previous updates into account as well and generally leads to faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0210fdca-ad08-4a76-bcc0-1b4b627d0e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3577344999909401\n",
      "1 0.2550213589310646\n"
     ]
    }
   ],
   "source": [
    "model = Mnist_CNN()\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bf2e72-db4d-47fd-a111-0fdaaf7a1192",
   "metadata": {},
   "source": [
    "### Using `nn.Sequential`\n",
    "\n",
    "`torch.nn` has another handy class we can use to simplify our code: `Sequential`.  \n",
    "A `Sequential` object runs each of the modules contained within it, in a **sequential manner**.  \n",
    "This is a simpler way of writing our neural network.\n",
    "\n",
    "To take advantage of this, we need to be able to **easily define a custom layer** from a given function.  \n",
    "For instance, PyTorch doesn’t have a `view` layer, and we need to create one for our network.  \n",
    "`Lambda` will create a layer that we can then use when defining a network with `Sequential`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9fc73423-0ddf-4cf3-94d0-e7dbe8c7b54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551da4dc-7a2f-4604-8129-361a6594d224",
   "metadata": {},
   "source": [
    "The model created with Sequential is simple:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "715df69c-5e2d-4533-9d2f-e924ecf96857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3102222809791565\n",
      "1 0.26368377068042753\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    Lambda(preprocess),\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(4),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985b9113-0569-45a4-9fb5-cf5c35f993fa",
   "metadata": {},
   "source": [
    "### Wrapping DataLoader\n",
    "\n",
    "Our CNN is fairly concise, but it only works with **MNIST**, because:\n",
    "\n",
    "- It assumes the input is a **28×28 long vector**\n",
    "- It assumes that the final CNN grid size is **4×4** (since that’s the average pooling kernel size we used)\n",
    "\n",
    "Let’s get rid of these two assumptions, so our model works with **any 2D single-channel image**.\n",
    "\n",
    "First, we can remove the initial `Lambda` layer by **moving the data preprocessing into a generator**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "59629370-5c74-4349-8c60-b38372e14990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3553f0bd-0c6b-4e87-a23c-e24e2f8ee012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            yield (self.func(*b))\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c9518b-c339-4735-8a0e-acd08ea81163",
   "metadata": {},
   "source": [
    "Next, we can replace nn.AvgPool2d with nn.AdaptiveAvgPool2d, which allows us to define the size of the output tensor we want, rather than the input tensor we have. As a result, our model will work with any size input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f64b6227-26fb-4656-a050-332a70df21bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b93446de-a8d2-4f7e-874a-263cf28b0d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.22151797465085984\n",
      "1 0.17144094106853008\n"
     ]
    }
   ],
   "source": [
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fca9c1-1717-4cb7-9525-c726e3c844cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_gpu_env]",
   "language": "python",
   "name": "conda-env-torch_gpu_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
